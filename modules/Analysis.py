import streamlit as st
import pandas as pd
import numpy as np
import os
import zipfile
import io
import re
import unicodedata
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from xgboost import XGBClassifier

import joblib


# =========================================================
# üîß TI·ªÄN X·ª¨ L√ù TI·∫æNG VI·ªÜT
# =========================================================

STOPWORDS = set("""
v√† l√† c·ªßa nh·ªØng c√°i c√°c m·ªôt trong ƒë∆∞·ª£c ƒë·ªÉ v·ªõi t·ª´ khi m√† th√¨ l√† ƒë·ªÅu n√†y kia ho·∫∑c n√™n n·∫øu tuy v√¨ nh∆∞ng v·∫≠y c√≤n r·∫•t l·∫°i ƒë√£ ƒëang s·∫Ω
""".split())

def normalize_unicode(text):
    return unicodedata.normalize("NFC", text)

def clean_regex(text):
    text = re.sub(r"[^a-zA-Z0-9√Ä-·ªπ\s]", " ", text)
    text = re.sub(r"\s+", " ", text)
    return text.strip()

def remove_stopwords(words):
    return " ".join([w for w in words.split() if w not in STOPWORDS])

def preprocess_text(text):
    if not isinstance(text, str):
        return ""
    text = normalize_unicode(text)
    text = text.lower()
    text = clean_regex(text)
    text = remove_stopwords(text)
    return text


# =========================================================
# üì¶ H√ÄM ƒê·ªåC DATA T·ª™ ZIP M·∫™U
# =========================================================

def generate_sample_zip():
    files = {
        "politics_01.txt": "Ch√≠nh ph·ªß v·ª´a th√¥ng qua ngh·ªã quy·∫øt m·ªõi v·ªÅ ph√°t tri·ªÉn kinh t·∫ø s·ªë.",
        "education_01.txt": "B·ªô Gi√°o d·ª•c c√¥ng b·ªë ƒë·ªïi m·ªõi ch∆∞∆°ng tr√¨nh h·ªçc ph·ªï th√¥ng.",
        "weather_01.txt": "Mi·ªÅn B·∫Øc r√©t ƒë·∫≠m do ·∫£nh h∆∞·ªüng kh√¥ng kh√≠ l·∫°nh.",
        "sports_01.txt": "Vi·ªát Nam th·∫Øng 3-1 trong tr·∫≠n giao h·ªØu."
    }

    mem = io.BytesIO()
    with zipfile.ZipFile(mem, "w") as z:
        for fn, content in files.items():
            z.writestr(fn, content)

    mem.seek(0)
    return mem


# =========================================================
# üì• ƒê·ªåC FOLDER TXT ‚Äì B·∫¢N PRO
# =========================================================

def read_txt_folder(files):
    rows = []

    for f in files:
        if f.name.endswith(".txt"):

            base = os.path.splitext(f.name)[0]
            parts = base.split(".")
            label = parts[-1].strip().upper()

            content = f.read().decode("utf-8", errors="ignore")
            lines = [line.strip() for line in content.split("\n") if line.strip()]

            for line in lines:
                rows.append([line, label])

    return pd.DataFrame(rows, columns=["text", "label"])


# =========================================================
# üì• ƒê·ªåC ZIP ‚Äì T·ª∞ NH·∫¨N LABEL
# =========================================================

def read_txt_zip(file):
    rows = []
    with zipfile.ZipFile(file, "r") as z:

        for fn in z.namelist():
            if fn.endswith(".txt"):

                base = os.path.splitext(fn)[0]
                parts = base.split("_")
                label = parts[0].upper()

                text = z.read(fn).decode("utf-8", errors="ignore")
                lines = [line.strip() for line in text.split("\n") if line.strip()]

                for line in lines:
                    rows.append([line, label])

    return pd.DataFrame(rows, columns=["text", "label"])


# =========================================================
# üß† GIAO DI·ªÜN CH√çNH
# =========================================================

def show():

    st.markdown("### üß† Analysis ‚Äì Train m√¥ h√¨nh ph√¢n lo·∫°i tin t·ª©c (B·∫£n PRO)")

    st.download_button(
        "‚¨áÔ∏è T·∫£i ZIP m·∫´u (4 m·∫´u nh·ªè)",
        data=generate_sample_zip(),
        file_name="sample_news.zip",
        mime="application/zip"
    )

    st.write("---")
    st.header("1Ô∏è‚É£ Upload d·ªØ li·ªáu")

    mode = st.radio(
        "Ch·ªçn ch·∫ø ƒë·ªô t·∫£i d·ªØ li·ªáu:",
        ["Folder TXT", "ZIP TXT", "CSV / Excel"],
        horizontal=True
    )

    if "df" not in st.session_state:
        st.session_state.df = None

    df = None

    # --- FOLDER TXT ---
    if mode == "Folder TXT":
        files = st.file_uploader("Ch·ªçn nhi·ªÅu file TXT", type=["txt"], accept_multiple_files=True)
        if files:
            df = read_txt_folder(files)
            st.session_state.df = df
            st.success(f"‚úî ƒê√£ ƒë·ªçc {len(df)} d√≤ng tin t·ª©c!")
            st.dataframe(df)

    # --- ZIP TXT ---
    elif mode == "ZIP TXT":
        up = st.file_uploader("Upload ZIP", type=["zip"])
        if up:
            df = read_txt_zip(up)
            st.session_state.df = df
            st.success(f"‚úî ZIP ƒë√£ ƒë·ªçc th√†nh c√¥ng ({len(df)} d√≤ng)!")
            st.dataframe(df)

    # --- CSV / EXCEL ---
    else:
        up = st.file_uploader("Upload CSV/Excel", type=["csv", "xlsx"])
        if up:
            ext = up.name.split(".")[-1]
            df = pd.read_csv(up) if ext == "csv" else pd.read_excel(up)
            st.session_state.df = df
            st.success("‚úî File b·∫£ng ƒë√£ ƒë·ªçc th√†nh c√¥ng!")
            st.dataframe(df)

    # === CH·ª®C NƒÇNG M·ªöI: DOWNLOAD CSV ===
    if st.session_state.df is not None:
        csv = st.session_state.df.to_csv(index=False).encode("utf-8")
        st.download_button(
            "üì• T·∫£i xu·ªëng d·ªØ li·ªáu CSV",
            data=csv,
            file_name="dataset.csv",
            mime="text/csv"
        )

    st.write("---")


    # =========================================================
    # üìä PH√ÇN T√çCH NHANH DATASET
    # =========================================================
    if st.session_state.df is not None:

        st.subheader("üìä Th·ªëng k√™ d·ªØ li·ªáu theo Label")

        fig, ax = plt.subplots(figsize=(4, 2.5))
        sns.countplot(x=st.session_state.df["label"], ax=ax)

        plt.xticks(rotation=45)
        plt.tight_layout()

        st.pyplot(fig, use_container_width=False)

        # =========================================================
        # ‚≠ê EXTRA FUNCTION 1 ‚Äî Histogram ƒë·ªô d√†i c√¢u
        # =========================================================
        st.subheader("üìè Ph√¢n ph·ªëi ƒë·ªô d√†i c√¢u (Histogram)")

        st.session_state.df["length"] = st.session_state.df["text"].apply(lambda x: len(str(x).split()))

        fig2, ax2 = plt.subplots(figsize=(4, 2.5))
        ax2.hist(st.session_state.df["length"], bins=20, color="#1abc9c", edgecolor="black")
        ax2.set_title("Ph√¢n ph·ªëi s·ªë l∆∞·ª£ng t·ª´ trong c√¢u")
        ax2.set_xlabel("S·ªë t·ª´")
        ax2.set_ylabel("S·ªë c√¢u")
        plt.tight_layout()

        st.pyplot(fig2, use_container_width=False)

        # =========================================================
        # ‚≠ê EXTRA FUNCTION 2 ‚Äî WordCloud cho t·ª´ng label
        # =========================================================
        from wordcloud import WordCloud

        st.subheader("‚òÅÔ∏è WordCloud theo t·ª´ng nh√£n")

        labels = st.session_state.df["label"].unique()

        for lb in labels:
            subset = st.session_state.df[st.session_state.df["label"] == lb]

            text_blob = " ".join(subset["text"].astype(str).tolist())

            if len(text_blob.strip()) < 5:
                st.warning(f"‚ö† Kh√¥ng ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ t·∫°o WordCloud cho label: {lb}")
                continue

            wc = WordCloud(
                width=600,
                height=300,
                background_color="white",
                colormap="viridis"
            ).generate(text_blob)

            fig_wc, ax_wc = plt.subplots(figsize=(6, 3))
            ax_wc.imshow(wc, interpolation="bilinear")
            ax_wc.axis("off")

            st.markdown(f"### üè∑ WordCloud ‚Äì {lb}")
            st.pyplot(fig_wc, use_container_width=False)

    st.write("---")
    st.header("2Ô∏è‚É£ Train model")


    model_choice = st.selectbox(
        "Ch·ªçn model:",
        ["Auto (XGBoost)", "XGBoost", "Logistic Regression", "SVM"]
    )

    status = st.empty()

    # =========================================================
    # üöÄ TRAIN MODEL
    # =========================================================

    if st.button("üöÄ Train"):

        df = st.session_state.df
        if df is None or len(df) < 10:
            st.error("‚ùå Dataset qu√° nh·ªè. C·∫ßn √≠t nh·∫•t 10 d√≤ng tin t·ª©c.")
            return

        df["text"] = df["text"].apply(preprocess_text)

        le = LabelEncoder()
        y = le.fit_transform(df["label"])
        X = df["text"].values

        class_counts = pd.Series(y).value_counts()
        if class_counts.min() < 2:
            st.error("‚ùå M·ªói nh√£n c·∫ßn t·ªëi thi·ªÉu 2 m·∫´u ƒë·ªÉ train.")
            return

        status.info("üîÑ ƒêang t·∫°o ƒë·∫∑c tr∆∞ng TF-IDF...")

        vectorizer = TfidfVectorizer(
            max_features=7000,
            ngram_range=(1, 2),
            min_df=1
        )
        X_vec = vectorizer.fit_transform(X)

        stratify_flag = y if class_counts.min() >= 3 else None

        status.info("üî• Training... vui l√≤ng ƒë·ª£i...")

        X_train, X_test, y_train, y_test = train_test_split(
            X_vec, y,
            test_size=0.25,
            stratify=stratify_flag,
            random_state=42
        )

        if model_choice in ["Auto (XGBoost)", "XGBoost"]:
            model = XGBClassifier(
                n_estimators=350,
                learning_rate=0.08,
                max_depth=10,
                subsample=0.9,
                colsample_bytree=0.9,
                eval_metric="mlogloss"
            )
        elif model_choice == "Logistic Regression":
            model = LogisticRegression(max_iter=5000)
        else:
            model = SVC(kernel="linear", probability=True)

        model.fit(X_train, y_train)
        preds = model.predict(X_test)

        acc = accuracy_score(y_test, preds)
        status.success(f"üéØ Accuracy: **{acc:.4f}**")

        os.makedirs("export", exist_ok=True)
        joblib.dump(model, "export/model.pkl")
        joblib.dump(vectorizer, "export/vectorizer.pkl")
        joblib.dump(le, "export/label_encoder.pkl")

        st.success("üì¶ Model ƒë√£ l∆∞u th√†nh c√¥ng v√†o th∆∞ m·ª•c export/!")

        # =========================================================
        # ‚≠ê EXTRA FUNCTION 3 ‚Äî EXPORT MODEL ZIP
        # =========================================================
        import zipfile

        with zipfile.ZipFile("export/model_package.zip", "w") as z:
            z.write("export/model.pkl")
            z.write("export/vectorizer.pkl")
            z.write("export/label_encoder.pkl")

        with open("export/model_package.zip", "rb") as f:
            st.download_button(
                "üì• T·∫£i Model.zip",
                data=f,
                file_name="model_package.zip",
                mime="application/zip"
            )

        # =========================================================
        # ‚≠ê EXTRA FUNCTION 4 ‚Äî Training Report
        # =========================================================
        from sklearn.metrics import confusion_matrix, classification_report
        import json

        # Accuracy bar chart
        fig_acc, ax_acc = plt.subplots(figsize=(4, 2.5))
        ax_acc.bar(["Accuracy"], [acc], color="#1abc9c")
        ax_acc.set_ylim(0, 1)
        plt.tight_layout()
        fig_acc.savefig("export/accuracy.png")
        st.image("export/accuracy.png", caption="üéØ Accuracy", width=450)

        # Confusion Matrix
        cm = confusion_matrix(y_test, preds)
        fig_cm, ax_cm = plt.subplots(figsize=(4, 3))
        sns.heatmap(
            cm,
            annot=True,
            fmt="d",
            cmap="Blues",
            xticklabels=le.classes_,
            yticklabels=le.classes_
        )
        ax_cm.set_title("Confusion Matrix")
        plt.tight_layout()
        fig_cm.savefig("export/confusion_matrix.png")
        st.image("export/confusion_matrix.png", caption="üìå Confusion Matrix", width=450)

        # Text report
        report = classification_report(y_test, preds, target_names=le.classes_)
        with open("export/report.txt", "w", encoding="utf-8") as f:
            f.write(report)
        st.code(report, language="text")

        # Save training metadata
        train_info = {
              "accuracy": float(acc),
              "model_name": str(model.__class__.__name__),
              "num_samples": len(df),
              "train_size": X_train.shape[0],   # FIXED
              "test_size": X_test.shape[0],     # FIXED
              "labels": list(le.classes_)
        }


        with open("export/train_info.json", "w", encoding="utf-8") as f:
            json.dump(train_info, f, indent=4, ensure_ascii=False)

        st.success("üìÅ ƒê√£ l∆∞u ƒë·∫ßy ƒë·ªß file b√°o c√°o t·∫°i export/")


    # =========================================================
    # üîÆ D·ª∞ B√ÅO
    # =========================================================
    st.write("---")
    st.header("3Ô∏è‚É£ D·ª± b√°o")

    txt = st.text_area("Nh·∫≠p n·ªôi dung tin t·ª©c...")

    if st.button("üîÆ D·ª± b√°o"):
        if not os.path.exists("export/model.pkl"):
            st.error("‚ùå Ch∆∞a c√≥ model. H√£y train tr∆∞·ªõc.")
            return

        model = joblib.load("export/model.pkl")
        vec = joblib.load("export/vectorizer.pkl")
        le = joblib.load("export/label_encoder.pkl")

        vec_txt = vec.transform([preprocess_text(txt)])
        pred = model.predict(vec_txt)[0]
        label = le.inverse_transform([pred])[0]

        st.success(f"‚û° K·∫øt qu·∫£ d·ª± b√°o: **{label}**")

    # =========================================================
    # ‚≠ê EXTRA FUNCTION 5 ‚Äî BATCH PREDICTION
    # =========================================================
    st.subheader("üìÇ D·ª± b√°o h√†ng lo·∫°t t·ª´ file CSV / TXT")

    batch_file = st.file_uploader("Upload file d·ª± b√°o", type=["txt", "csv"])

    if batch_file:
        ext = batch_file.name.split(".")[-1].lower()

        if ext == "txt":
            lines = batch_file.read().decode("utf-8").split("\n")
            df_batch = pd.DataFrame({"text": [l.strip() for l in lines if l.strip()]})

        elif ext == "csv":
            df_batch = pd.read_csv(batch_file)

        else:
            st.error("‚ùå Ch·ªâ h·ªó tr·ª£ TXT ho·∫∑c CSV.")
            return

        model = joblib.load("export/model.pkl")
        vec = joblib.load("export/vectorizer.pkl")
        le = joblib.load("export/label_encoder.pkl")

        df_batch["clean"] = df_batch["text"].apply(preprocess_text)
        Xb = vec.transform(df_batch["clean"])
        preds = model.predict(Xb)
        df_batch["label"] = le.inverse_transform(preds)

        st.dataframe(df_batch)

        csv_out = df_batch.to_csv(index=False).encode("utf-8")
        st.download_button(
            "üì• T·∫£i k·∫øt qu·∫£ d·ª± b√°o",
            data=csv_out,
            file_name="batch_prediction.csv",
            mime="text/csv"
        )
