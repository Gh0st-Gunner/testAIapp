import streamlit as st
import pandas as pd
import numpy as np
import os
import zipfile
import io
import re
import unicodedata
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from xgboost import XGBClassifier

import joblib


# =========================================================
# üîß TI·ªÄN X·ª¨ L√ù TI·∫æNG VI·ªÜT
# =========================================================

STOPWORDS = set("""
v√† l√† c·ªßa nh·ªØng c√°i c√°c m·ªôt trong ƒë∆∞·ª£c ƒë·ªÉ v·ªõi t·ª´ khi m√† th√¨ l√† ƒë·ªÅu n√†y kia ho·∫∑c n√™n n·∫øu tuy v√¨ nh∆∞ng v·∫≠y c√≤n r·∫•t l·∫°i ƒë√£ ƒëang s·∫Ω
""".split())

def normalize_unicode(text):
    return unicodedata.normalize("NFC", text)

def clean_regex(text):
    text = re.sub(r"[^a-zA-Z0-9√Ä-·ªπ\s]", " ", text)
    text = re.sub(r"\s+", " ", text)
    return text.strip()

def remove_stopwords(words):
    return " ".join([w for w in words.split() if w not in STOPWORDS])

def preprocess_text(text):
    if not isinstance(text, str):
        return ""
    text = normalize_unicode(text)
    text = text.lower()
    text = clean_regex(text)
    text = remove_stopwords(text)
    return text


# =========================================================
# üì¶ H√ÄM ƒê·ªåC DATA T·ª™ ZIP M·∫™U
# =========================================================

def generate_sample_zip():
    files = {
        "politics_01.txt": "Ch√≠nh ph·ªß v·ª´a th√¥ng qua ngh·ªã quy·∫øt m·ªõi v·ªÅ ph√°t tri·ªÉn kinh t·∫ø s·ªë.",
        "education_01.txt": "B·ªô Gi√°o d·ª•c c√¥ng b·ªë ƒë·ªïi m·ªõi ch∆∞∆°ng tr√¨nh h·ªçc ph·ªï th√¥ng.",
        "weather_01.txt": "Mi·ªÅn B·∫Øc r√©t ƒë·∫≠m do ·∫£nh h∆∞·ªüng kh√¥ng kh√≠ l·∫°nh.",
        "sports_01.txt": "Vi·ªát Nam th·∫Øng 3-1 trong tr·∫≠n giao h·ªØu."
    }

    mem = io.BytesIO()
    with zipfile.ZipFile(mem, "w") as z:
        for fn, content in files.items():
            z.writestr(fn, content)

    mem.seek(0)
    return mem


# =========================================================
# üì• ƒê·ªåC FOLDER TXT ‚Äì B·∫¢N PRO
# =========================================================

def read_txt_folder(files):
    rows = []

    for f in files:
        if f.name.endswith(".txt"):

            # T√°ch nh√£n t·ª´ t√™n file
            # V√≠ d·ª• "1. SPORTS.txt" -> "SPORTS"
            base = os.path.splitext(f.name)[0]
            parts = base.split(".")
            label = parts[-1].strip().upper()

            # ƒê·ªçc file v√† t√°ch m·ªói d√≤ng th√†nh 1 m·∫´u
            content = f.read().decode("utf-8", errors="ignore")
            lines = [line.strip() for line in content.split("\n") if line.strip()]

            for line in lines:
                rows.append([line, label])

    return pd.DataFrame(rows, columns=["text", "label"])


# =========================================================
# üì• ƒê·ªåC ZIP ‚Äì T·ª∞ NH·∫¨N LABEL
# =========================================================

def read_txt_zip(file):
    rows = []
    with zipfile.ZipFile(file, "r") as z:

        for fn in z.namelist():
            if fn.endswith(".txt"):

                base = os.path.splitext(fn)[0]
                parts = base.split("_")
                label = parts[0].upper()

                text = z.read(fn).decode("utf-8", errors="ignore")
                lines = [line.strip() for line in text.split("\n") if line.strip()]

                for line in lines:
                    rows.append([line, label])

    return pd.DataFrame(rows, columns=["text", "label"])


# =========================================================
# üß† GIAO DI·ªÜN CH√çNH
# =========================================================

def show():

    st.markdown("### üß† Analysis ‚Äì Train m√¥ h√¨nh ph√¢n lo·∫°i tin t·ª©c (B·∫£n PRO)")

    st.download_button(
        "‚¨áÔ∏è T·∫£i ZIP m·∫´u (4 m·∫´u nh·ªè)",
        data=generate_sample_zip(),
        file_name="sample_news.zip",
        mime="application/zip"
    )

    st.write("---")
    st.header("1Ô∏è‚É£ Upload d·ªØ li·ªáu")

    mode = st.radio(
        "Ch·ªçn ch·∫ø ƒë·ªô t·∫£i d·ªØ li·ªáu:",
        ["Folder TXT", "ZIP TXT", "CSV / Excel"],
        horizontal=True
    )

    if "df" not in st.session_state:
        st.session_state.df = None

    df = None

    # --- FOLDER TXT ---
    if mode == "Folder TXT":
        files = st.file_uploader("Ch·ªçn nhi·ªÅu file TXT", type=["txt"], accept_multiple_files=True)
        if files:
            df = read_txt_folder(files)
            st.session_state.df = df
            st.success(f"‚úî ƒê√£ ƒë·ªçc {len(df)} d√≤ng tin t·ª©c!")
            st.dataframe(df)

    # --- ZIP TXT ---
    elif mode == "ZIP TXT":
        up = st.file_uploader("Upload ZIP", type=["zip"])
        if up:
            df = read_txt_zip(up)
            st.session_state.df = df
            st.success(f"‚úî ZIP ƒë√£ ƒë·ªçc th√†nh c√¥ng ({len(df)} d√≤ng)!")
            st.dataframe(df)

    # --- CSV / EXCEL ---
    else:
        up = st.file_uploader("Upload CSV/Excel", type=["csv", "xlsx"])
        if up:
            ext = up.name.split(".")[-1]
            df = pd.read_csv(up) if ext == "csv" else pd.read_excel(up)
            st.session_state.df = df
            st.success("‚úî File b·∫£ng ƒë√£ ƒë·ªçc th√†nh c√¥ng!")
            st.dataframe(df)

    st.write("---")

    # =========================================================
    # üìä PH√ÇN T√çCH NHANH DATASET
    # =========================================================
    if st.session_state.df is not None:

        st.subheader("üìä Th·ªëng k√™ d·ªØ li·ªáu theo Label")

        # fig, ax = plt.subplots(figsize=(6, 4))
        # sns.countplot(x=st.session_state.df["label"], ax=ax)
        # plt.xticks(rotation=45)
        # st.pyplot(fig)
        fig, ax = plt.subplots(figsize=(4, 2.5))   # Thu nh·ªè figure
        sns.countplot(x=st.session_state.df["label"], ax=ax)

        plt.xticks(rotation=45)
        plt.tight_layout()

        # Quan tr·ªçng: kh√¥ng cho Streamlit ph√≥ng r·ªông!
        st.pyplot(fig, use_container_width=False)


    st.write("---")
    st.header("2Ô∏è‚É£ Train model")

    model_choice = st.selectbox(
        "Ch·ªçn model:",
        ["Auto (XGBoost)", "XGBoost", "Logistic Regression", "SVM"]
    )

    status = st.empty()

    # =========================================================
    # üöÄ TRAIN MODEL
    # =========================================================

    if st.button("üöÄ Train"):

        df = st.session_state.df
        if df is None or len(df) < 10:
            st.error("‚ùå Dataset qu√° nh·ªè. C·∫ßn √≠t nh·∫•t 10 d√≤ng tin t·ª©c.")
            return

        # Ti·ªÅn x·ª≠ l√Ω text
        df["text"] = df["text"].apply(preprocess_text)

        # Label Encoder
        le = LabelEncoder()
        y = le.fit_transform(df["label"])
        X = df["text"].values

        # Ki·ªÉm tra s·ªë m·∫´u m·ªói class
        class_counts = pd.Series(y).value_counts()
        if class_counts.min() < 2:
            st.error("‚ùå M·ªói nh√£n c·∫ßn t·ªëi thi·ªÉu 2 m·∫´u ƒë·ªÉ train.")
            return

        # TF-IDF
        status.info("üîÑ ƒêang t·∫°o ƒë·∫∑c tr∆∞ng TF-IDF...")

        vectorizer = TfidfVectorizer(
            max_features=7000,
            ngram_range=(1, 2),
            min_df=1
        )
        X_vec = vectorizer.fit_transform(X)

        # Ch·ªçn test size t·ª± ƒë·ªông
        test_size = 0.25
        stratify_flag = y if class_counts.min() >= 3 else None

        status.info("üî• Training... vui l√≤ng ƒë·ª£i...")

        X_train, X_test, y_train, y_test = train_test_split(
            X_vec, y,
            test_size=test_size,
            stratify=stratify_flag,
            random_state=42
        )

        # MODEL
        if model_choice in ["Auto (XGBoost)", "XGBoost"]:
            model = XGBClassifier(
                n_estimators=350,
                learning_rate=0.08,
                max_depth=10,
                subsample=0.9,
                colsample_bytree=0.9,
                eval_metric="mlogloss"
            )
        elif model_choice == "Logistic Regression":
            model = LogisticRegression(max_iter=5000)
        else:
            model = SVC(kernel="linear", probability=True)

        # TRAIN
        model.fit(X_train, y_train)
        preds = model.predict(X_test)

        acc = accuracy_score(y_test, preds)
        status.success(f"üéØ Accuracy: **{acc:.4f}**")

        # SAVE MODEL
        os.makedirs("export", exist_ok=True)
        joblib.dump(model, "export/model.pkl")
        joblib.dump(vectorizer, "export/vectorizer.pkl")
        joblib.dump(le, "export/label_encoder.pkl")

        st.success("üì¶ Model ƒë√£ l∆∞u th√†nh c√¥ng v√†o th∆∞ m·ª•c export/!")

    # =========================================================
    # üîÆ D·ª∞ B√ÅO
    # =========================================================
    st.write("---")
    st.header("3Ô∏è‚É£ D·ª± b√°o")

    txt = st.text_area("Nh·∫≠p n·ªôi dung tin t·ª©c...")

    if st.button("üîÆ D·ª± b√°o"):
        if not os.path.exists("export/model.pkl"):
            st.error("‚ùå Ch∆∞a c√≥ model. H√£y train tr∆∞·ªõc.")
            return

        model = joblib.load("export/model.pkl")
        vec = joblib.load("export/vectorizer.pkl")
        le = joblib.load("export/label_encoder.pkl")

        vec_txt = vec.transform([preprocess_text(txt)])
        pred = model.predict(vec_txt)[0]
        label = le.inverse_transform([pred])[0]

        st.success(f"‚û° K·∫øt qu·∫£ d·ª± b√°o: **{label}**")
