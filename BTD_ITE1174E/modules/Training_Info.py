import streamlit as st
import pandas as pd
import joblib
import os
import re
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.metrics import classification_report, confusion_matrix, f1_score


# ============================================================
# 1. H√ÄM KI·ªÇM TRA D·ªÆ LI·ªÜU TH√î
# ============================================================

def load_and_validate_raw_data():
    path = "raw_news.csv"

    if not os.path.exists(path):
        return None, ["‚ö† Kh√¥ng t√¨m th·∫•y file raw_news.csv!"]

    errors = []
    df = None

    try:
        df = pd.read_csv(path)

        # 1) File r·ªóng
        if df.empty:
            errors.append("‚ùå File raw_news.csv r·ªóng.")
            return None, errors

        # 2) Thi·∫øu c·ªôt
        required_cols = ["text", "label"]
        for col in required_cols:
            if col not in df.columns:
                errors.append(f"‚ùå Thi·∫øu c·ªôt b·∫Øt bu·ªôc: {col}")

        if errors:
            return None, errors

        # 3) Null check
        if df["text"].isna().sum() > 0:
            errors.append("‚ùå C√≥ d√≤ng b·ªã null trong c·ªôt TEXT.")

        if df["label"].isna().sum() > 0:
            errors.append("‚ùå C√≥ d√≤ng b·ªã null trong c·ªôt LABEL.")

        # 4) D√≤ng r·ªóng ho·∫∑c to√†n k√Ω t·ª± tr·∫Øng
        empty_rows = df["text"].str.strip().eq("").sum()
        if empty_rows > 0:
            errors.append(f"‚ùå C√≥ {empty_rows} d√≤ng text b·ªã r·ªóng.")

        # 5) K√Ω t·ª± ƒë·∫∑c bi·ªát ‚Üí flag
        pattern_special = r"[^0-9A-Za-z√Ä-·ªπ\s\.,!?%-]"
        special_rows = df["text"].str.contains(pattern_special, regex=True).sum()
        if special_rows > 0:
            errors.append(
                f"‚ö† Ph√°t hi·ªán {special_rows} d√≤ng ch·ª©a k√Ω t·ª± ƒë·∫∑c bi·ªát (emoji, k√Ω t·ª± l·∫°...)."
            )

        # 6) Text qu√° ng·∫Øn
        short_rows = df[df["text"].str.len() < 5]
        if len(short_rows) > 0:
            errors.append(f"‚ö† C√≥ {len(short_rows)} c√¢u qu√° ng·∫Øn (<5 k√Ω t·ª±).")

        # 7) Tr√πng l·∫∑p
        dup = df.duplicated().sum()
        if dup > 0:
            errors.append(f"‚ö† C√≥ {dup} d√≤ng tr√πng l·∫∑p trong d·ªØ li·ªáu.")

        # 8) √çt h∆°n 2 nh√£n ‚Üí kh√¥ng th·ªÉ train
        if df["label"].nunique() < 2:
            errors.append("‚ùå File ch·ªâ c√≥ 1 nh√£n ‚Üí kh√¥ng th·ªÉ train model!")

        return df, errors

    except Exception as e:
        return None, [f"‚ùå L·ªói ƒë·ªçc file: {e}"]


# ============================================================
# 2. TI·ªÄN X·ª¨ L√ù TEXT
# ============================================================

def clean_text(text):
    if not isinstance(text, str):
        return ""
    text = text.lower().strip()
    text = re.sub(r"\s+", " ", text)
    return text


# ============================================================
# PAGE MAIN
# ============================================================

def show():

    st.markdown(
        "<h3 style='color:blue;'>Training Info ‚Äì Th√¥ng s·ªë Hu·∫•n luy·ªán M√°y t√≠nh</h3>",
        unsafe_allow_html=True,
    )
    st.write("---")

    # ============================================================
    # 1. HI·ªÇN TH·ªä D·ªÆ LI·ªÜU TH√î + KI·ªÇM TRA L·ªñI
    # ============================================================

    st.write("## 1. D·ªØ li·ªáu th√¥")

    df_raw, issues = load_and_validate_raw_data()

    # ∆Øu ti√™n d·ªØ li·ªáu user upload t·ª´ Analysis.py
    if "df" in st.session_state and st.session_state.df is not None:
        df_raw = st.session_state.df
        issues = []
        st.success("‚úî ƒêang d√πng d·ªØ li·ªáu tr·ª±c ti·∫øp t·ª´ ng∆∞·ªùi d√πng upload (Analysis.py).")

    # Hi·ªán c√°c c·∫£nh b√°o
    if issues:
        for msg in issues:
            st.warning(msg)

    if df_raw is None:
        st.stop()

    df_raw["length"] = df_raw["text"].str.len()
    st.dataframe(df_raw.head())

    st.write("---")

    # ============================================================
    # 2. HI·ªÇN TH·ªä D·ªÆ LI·ªÜU SAU TI·ªÄN X·ª¨ L√ù
    # ============================================================

    st.write("## 2. D·ªØ li·ªáu sau ti·ªÅn x·ª≠ l√Ω")

    df_processed = df_raw.copy()
    df_processed["clean_text"] = df_processed["text"].apply(clean_text)

    st.dataframe(df_processed.head())

    st.write("---")

    # ============================================================
    # 3. HI·ªÇN TH·ªä ƒê∆Ø·ªúNG D·∫™N MODEL + VECTOR
    # ============================================================

    st.write("## 3. ƒê∆∞·ªùng d·∫´n Model & Vectorizer")

    model_path = "export/model.pkl"
    vec_path = "export/vectorizer.pkl"

    if not os.path.exists(model_path) or not os.path.exists(vec_path):
        st.error("‚ùå Kh√¥ng t√¨m th·∫•y model.pkl ho·∫∑c vectorizer.pkl ‚Üí ch∆∞a train model.")
        st.stop()

    st.success(f"‚úî Model: {model_path}")
    st.success(f"‚úî Vectorizer: {vec_path}")

    st.write("---")

    # ============================================================
    # 4. TH√îNG TIN MODEL ƒê√É HU·∫§N LUY·ªÜN
    # ============================================================

    st.write("## 4. Th√¥ng tin Model ƒë√£ hu·∫•n luy·ªán")

    model = joblib.load(model_path)
    st.code(str(model))

    st.write("---")

    # ============================================================
    # 5. HI·ªÇN TH·ªä TRAIN_INFO.JSON
    # ============================================================

    st.write("## 5. Th√¥ng tin file train_info.json")

    train_info = "export/train_info.json"
    if os.path.exists(train_info):
        import json

        st.json(json.load(open(train_info)))
        st.success("‚úî ƒê·ªçc file train_info.json th√†nh c√¥ng.")
    else:
        st.warning("‚ö† Kh√¥ng c√≥ train_info.json ‚Äî h√£y train l·∫°i model!")

    st.write("---")

    # ============================================================
    # 6. ƒê√ÅNH GI√Å M√î H√åNH B·∫∞NG MACRO F1-SCORE
    # ============================================================

    st.write("## 6. ƒê√°nh gi√° m√¥ h√¨nh b·∫±ng Macro F1-score")

    vectorizer = joblib.load(vec_path)
    X = vectorizer.transform(df_processed["clean_text"])

    # √âp nh√£n v·ªÅ string ƒë·ªÉ tr√°nh l·ªói mix ki·ªÉu
    y = df_processed["label"].astype(str)

    # D·ª± b√°o v·ªõi model ch√≠nh v√† √©p v·ªÅ string
    preds = pd.Series(model.predict(X)).astype(str)

    # F1-score
    f1 = f1_score(y, preds, average="macro")
    st.success(f"üî• Macro F1-score: {f1:.4f}")

    # B√°o c√°o chi ti·∫øt
    st.text(classification_report(y, preds))

    # Confusion matrix
    cm = confusion_matrix(y, preds)
    fig, ax = plt.subplots(figsize=(6, 4))
    sns.heatmap(cm, annot=True, cmap="Blues", ax=ax, fmt="d")
    st.pyplot(fig)

    st.write("---")

    # ============================================================
    # 7. SO S√ÅNH 3 M√î H√åNH: XGBoost ‚Äì SVM ‚Äì Logistic Regression
    #    (t√≠nh Macro F1-score, ƒë√£ FIX label cho XGBoost)
    # ============================================================

    st.write("## 7. So s√°nh c√°c m√¥ h√¨nh ML (Macro F1-score)")

    from sklearn.model_selection import train_test_split
    from sklearn.svm import SVC
    from sklearn.linear_model import LogisticRegression
    from sklearn.preprocessing import LabelEncoder
    from xgboost import XGBClassifier

    # D√πng LabelEncoder ƒë·ªÉ m√£ ho√° nh√£n sang s·ªë cho c·∫£ 3 m√¥ h√¨nh
    le = LabelEncoder()
    y_encoded = le.fit_transform(y)

    X_train, X_test, y_train, y_test = train_test_split(
        X, y_encoded, test_size=0.25, random_state=42
    )

    # XGBoost c·∫ßn dense
    X_train_dense = X_train.toarray()
    X_test_dense = X_test.toarray()

    results = {}

    # XGBoost
    try:
        xgb = XGBClassifier(eval_metric="mlogloss")
        xgb.fit(X_train_dense, y_train)
        preds_xgb = xgb.predict(X_test_dense)
        results["XGBoost"] = f1_score(y_test, preds_xgb, average="macro")
    except Exception as e:
        results["XGBoost"] = 0.0
        st.error(f"L·ªói XGBoost: {e}")

    # SVM
    svm = SVC(kernel="linear")
    svm.fit(X_train, y_train)
    preds_svm = svm.predict(X_test)
    results["SVM"] = f1_score(y_test, preds_svm, average="macro")

    # Logistic Regression
    lr = LogisticRegression(max_iter=3000)
    lr.fit(X_train, y_train)
    preds_lr = lr.predict(X_test)
    results["Logistic Regression"] = f1_score(y_test, preds_lr, average="macro")

    # V·∫Ω bi·ªÉu ƒë·ªì
    fig, ax = plt.subplots(figsize=(8, 4))
    ax.bar(results.keys(), results.values(), color=["#007bff", "#ff7f0e", "#28a745"])
    ax.set_ylabel("Macro F1-score")
    ax.set_ylim(0, 1)
    ax.set_title("So s√°nh F1-score gi·ªØa c√°c m√¥ h√¨nh ML")
    st.pyplot(fig)

    # L∆∞u file
    os.makedirs("export", exist_ok=True)
    fig.savefig("export/images.png")
    st.success("‚úî ƒê√£ l∆∞u ·∫£nh bi·ªÉu ƒë·ªì v√†o export/images.png")

    st.json(results)

    st.write("---")

    # ============================================================
    # 8. K·∫æT LU·∫¨N
    # ============================================================

    st.write("## 8. K·∫øt lu·∫≠n")
    st.info(
        """
    ‚úî D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c ki·ªÉm tra v√† l√†m s·∫°ch.  
    ‚úî M√¥ h√¨nh hi·ªán t·∫°i ho·∫°t ƒë·ªông ·ªïn ƒë·ªãnh v·ªõi F1-score cao.  
    ‚úî XGBoost / SVM / Logistic Regression c√≥ th·ªÉ d√πng l√†m baseline.  
    ‚úî C√≥ th·ªÉ n√¢ng c·∫•p b·∫±ng BERT / PhoBERT ƒë·ªÉ c·∫£i thi·ªán ch·∫•t l∆∞·ª£ng.  
    """
    )
